{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4839e577-d4b6-4a64-a8d9-a7dacc7a9f5d",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. **Introduction to Machine Learning**\n",
    "    - [Objective](#ml-objective)\n",
    "    - [Evaluation Metrics](#evaluation-metrics)\n",
    "      \n",
    "2. **Data Loading**\n",
    "    - [Importing Libraries](#Importing-Libraries)\n",
    "    - [Loading the Dataset](#Loading-the-Dataset)\n",
    "\n",
    "3. **Feature Definition**\n",
    "    - [Defining Numeric and Categorical Features](#Defining-Numeric-and-Categorical-Features)\n",
    "\n",
    "4. **Data Splitting**\n",
    "    - [Separating Features (X) and Target Variable (y)](#Separating-Features-and-Target-Variable)\n",
    "\n",
    "5. **Data Preprocessing**\n",
    "    - [Preprocessing Steps for Numeric and Categorical Features](#Preprocessing-Steps-for-Numeric-and-Categorical-Features)\n",
    "    - [Combining Preprocessing Steps](#Combining-Preprocessing-Steps)\n",
    "\n",
    "6. **Model Definition**\n",
    "    - [Creating the Time Series Split](#Creating-the-Time-Series-Split)\n",
    "    - [Defining the XGBoost Model](#Defining-the-XGBoost-Model)\n",
    "\n",
    "7. **Pipeline Creation**\n",
    "    - [Defining the Pipeline](#Defining-the-Pipeline)\n",
    "\n",
    "8. **Hyperparameter Tuning**\n",
    "    - [Defining the Hyperparameter Space](#Defining-the-Hyperparameter-Space)\n",
    "    - [Creating and Fitting the RandomizedSearchCV](#Creating-and-Fitting-the-RandomizedSearchCV)\n",
    "\n",
    "9. **Model Evaluation**\n",
    "    - [Performing Cross-Validation on the Best Model](#Performing-Cross-Validation-on-the-Best-Model)\n",
    "\n",
    "10. **Final Model Fitting and Predictions**\n",
    "    - [Fitting the Best Model on the Training Data](#Fitting-the-Best-Model-on-the-Training-Data)\n",
    "    - [Making Predictions on the Test Data Using Best Model](#Making-Predictions-on-the-Test-Data-Using-Best-Model)\n",
    "    - [Calculating and Printing Error Metrics](#Calculating-and-Printing-Error-Metrics)\n",
    "\n",
    "11. **Feature Importance**\n",
    "    - [Computing Feature Importance](#Computing-Feature-Importance)\n",
    "\n",
    "12. **Results**\n",
    "    - [Making Predictions on the Entire Data](#Making-Predictions-on-the-entire-Data)\n",
    "    - [Creating a Dataset with True and Predicted Demand](#Creating-a-Dataset-with-True-and-Predicted-Demand)\n",
    "\n",
    "13. **Results Visualization**\n",
    "    - [Plotting Hourly Electricity Demand: True vs Predicted](#Plotting-Hourly-Electricity-Demand-True-vs-Predicted)\n",
    "\n",
    "14. **Filtering Data for Specific Day**\n",
    "    - [Filtering Data for a Specific Day](#Filtering-Data-for-a-Specific-Day)\n",
    "\n",
    "15. **Conclusion**\n",
    "    - [Summary of Results](#Summary-of-Results)\n",
    "    - [Future Work](#Future-Work)\n",
    "    - [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031b63fe-b7d5-4330-b963-a5778d2eb422",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "## Introduction to Machine Learning\n",
    "\n",
    "### Objective\n",
    "<a id=\"ml-objective\"></a>\n",
    "The objective of utilizing machine learning on this dataset, specifically for electricity demand prediction, is to utilize historical data alongside pertinent features (like temperature, humidity, day of the week, holidays, etc.) to construct models capable of precisely predicting future patterns in electricity consumption.\n",
    "\n",
    "### Evaluation Metrics\n",
    "<a id=\"evaluation-metrics\"></a>\n",
    "\n",
    "In evaluating my model for predicting electricity demand, I utilized several key metrics to assess its performance:\n",
    "\n",
    "- **Mean Absolute Error (MAE)**: This metric quantifies the average magnitude of errors between predicted and actual values, providing a straightforward measure of prediction accuracy.\n",
    "  \n",
    "- **Mean Squared Error (MSE)** and **Root Mean Squared Error (RMSE)**: MSE calculates the average squared differences between predictions and actual values, while RMSE represents the square root of MSE, providing a measure of prediction error in the same units as the predicted values.\n",
    "\n",
    "- **R-squared (Coefficient of Determination)**: R-squared measures the proportion of the variance in the dependent variable (electricity demand) that is predictable from the independent variables (features), indicating how well the model fits the data.\n",
    "\n",
    "- **Mean Absolute Percentage Error (MAPE)**: MAPE offers a relative measure of prediction accuracy by calculating the average percentage deviation of predictions from actual values.\n",
    "\n",
    "These metrics collectively provide a comprehensive evaluation of the model's ability to predict electricity demand accurately, guiding decisions on model selection and performance optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2293b41-e400-4061-ba79-5b69b04e8da5",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "693e65e5-0505-4018-a894-fedfd365c610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (1.3.2)\n",
      "Requirement already satisfied: xgboost in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (2.1.0)\n",
      "Requirement already satisfied: matplotlib in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (3.8.2)\n",
      "Requirement already satisfied: seaborn in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from matplotlib) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from matplotlib) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ramesh/Data-Science/myenv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Library installation, xgboost needs to install carefully on Mac\n",
    "!pip install pandas numpy scikit-learn xgboost matplotlib seaborn pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5ce017-f1f7-4133-8f27-fb7db631de43",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/Data-Science/myenv/lib/python3.9/site-packages/pandas/__init__.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _dependency \u001b[38;5;129;01min\u001b[39;00m _hard_dependencies:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_dependency\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         _missing_dependencies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Data-Science/myenv/lib/python3.9/site-packages/numpy/__init__.py:130\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _distributor_init\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__config__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show \u001b[38;5;28;01mas\u001b[39;00m show_config\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    132\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mError importing numpy: you should not try to import numpy from\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124m    its source directory; please exit the numpy source tree, and relaunch\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124m    your python interpreter from there.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/Data-Science/myenv/lib/python3.9/site-packages/numpy/__config__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This file is generated by numpy's build process\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# It contains system_info results at the time of building this package.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     __cpu_features__,\n\u001b[1;32m      6\u001b[0m     __cpu_baseline__,\n\u001b[1;32m      7\u001b[0m     __cpu_dispatch__,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m _built_with_meson \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Data-Science/myenv/lib/python3.9/site-packages/numpy/core/__init__.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m         env_added\u001b[38;5;241m.\u001b[39mappend(envkey)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "File \u001b[0;32m~/Data-Science/myenv/lib/python3.9/site-packages/numpy/core/multiarray.py:10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mCreate the numpy.core.multiarray namespace for backward compatibility. In v1.16\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mthe multiarray and umath c-extension modules were merged into a single\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m~/Data-Science/myenv/lib/python3.9/site-packages/numpy/core/overrides.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_module\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getargspec\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n\u001b[1;32m     12\u001b[0m ARRAY_FUNCTIONS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     14\u001b[0m array_function_like_doc \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"like : array_like, optional\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m        Reference object to allow the creation of arrays which are not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m        compatible with that passed in via this argument.\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m )\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:203\u001b[0m, in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c7426-792e-4226-9b3a-431e109fcc10",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "<a id=\"Loading-the-Dataset\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab8b74-f421-4dc8-b4fd-31f2aa9156d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for ML\n",
    "df = pd.read_pickle('./data/electricity_data_pre_ml.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a9736a-adf8-451b-ab6a-ecd00f86be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the date as the index\n",
    "df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30949bca-6ba3-4990-94b5-f6d0d3d41ceb",
   "metadata": {},
   "source": [
    "## Feature Definition\n",
    "\n",
    "### Defining Numeric and Categorical Features\n",
    "<a id=\"Defining-Numeric-and-Categorical-Features\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925dbfc4-ad0f-4808-bb1a-ff3a4cc50ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "numeric_features = [\n",
    "    'HOEP', 'Temperature', 'Wind_Speed', 'Relative_Humidity', \n",
    "    'Pressure_Station', 'demand_lag_1', 'demand_lag_2', 'demand_lag_6', \n",
    "    'demand_lag_12', 'demand_lag_24', 'demand_lag_48', 'demand_lag_168', \n",
    "    'demand_rolling_mean_6', 'demand_rolling_mean_12', 'demand_rolling_mean_24', \n",
    "    'demand_rolling_mean_48', 'demand_rolling_mean_168', 'EWMA_6', 'EWMA_12', \n",
    "    'EWMA_24', 'EWMA_48', 'EWMA_168', 'Sin_Hour', 'Cos_Hour'\n",
    "]\n",
    "categorical_features = [\n",
    "    'Hour', 'DayOfWeek', 'IsWeekend', 'IsHoliday', 'Year', \n",
    "    'Month', 'Season'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aba33e-a7d2-4bc8-919f-f39405a7d2fb",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "### Separating Features (X) and Target Variable (y)\n",
    "<a id=\"Separating-Features-and-Target-Variable\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17632e8-c913-42a1-9fd2-99dbd8604abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X = df[numeric_features + categorical_features]\n",
    "y = df['Ontario_Demand']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42bc30-669e-4ff1-aefa-6e8565ee8933",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Preprocessing Steps for Numeric and Categorical Features\n",
    "<a id=\"Preprocessing-Steps-for-Numeric-and-Categorical-Features\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19259fad-0eaa-42f1-8057-8fcbba810db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing steps\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c795b-a59d-43ce-98a7-2765694e08b5",
   "metadata": {},
   "source": [
    "### Combining Preprocessing Steps\n",
    "<a id=\"Combining-Preprocessing-Steps\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b23d5-83da-4690-aa90-0442dc0657d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da20eb1-b464-435a-b625-fb9c2b106bbd",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "### Creating the Time Series Split\n",
    "<a id=\"Creating-the-Time-Series-Split\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a12649-fa2b-47e1-afa7-ce3ab68661b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the time series split\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap = 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b5d1d5-a6ba-423b-8a27-324fd97e5091",
   "metadata": {},
   "source": [
    "Implemented `TimeSeriesSplit` with `n_splits=5` and `gap=24` because it facilitates a robust evaluation of my model on sequential data. The gap parameter (set to 24) creates a buffer zone between training and test sets, preventing information from the immediate past (last 24 hours of the training set) from leaking into the test set. This ensures the model doesn't inadvertently learn from future data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98a059-bb57-494b-bad2-b3bb9ea0af9e",
   "metadata": {},
   "source": [
    "### Defining the XGBoost Model\n",
    "<a id=\"Defining-the-XGBoost-Model\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fc0c04-9b25-4477-866c-7220204119d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the XGBoost model with GPU acceleration\n",
    "# xgb_model = xgb.XGBRegressor(\n",
    "#     tree_method='gpu_hist',  # Use GPU for histogram-based learning\n",
    "#     gpu_id=0,  # Specify GPU device (use gpu_id=0 for the first GPU)\n",
    "#     random_state=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b52eb2-c4f1-4f43-bfc3-451479c63e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    tree_method='hist',  # Use CPU for histogram-based learning\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5f92b4-512f-4271-811d-f6ea1a34f98b",
   "metadata": {},
   "source": [
    "## Pipeline Creation\n",
    "\n",
    "### Defining the Pipeline\n",
    "<a id=\"Defining-the-Pipeline\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8d065-2b97-409b-977d-6dbca4ead39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb', xgb_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfca11a-060d-4be6-a32b-b89508d75e31",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "### Defining the Hyperparameter Space\n",
    "<a id=\"Defining-the-Hyperparameter-Space\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a522afb-229a-4ecf-a58f-6107a1f14c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter space\n",
    "param_space = {\n",
    "    'xgb__n_estimators': [250, 275, 300, 325, 350, 375, 400],\n",
    "    'xgb__max_depth': [6, 7, 8, 9],\n",
    "    'xgb__learning_rate': [0.05, 0.075, 0.1, 0.125, 0.15],\n",
    "    'xgb__subsample': [0.95, 0.975, 1.0],\n",
    "    'xgb__colsample_bytree': [0.7, 0.75, 0.8, 0.85, 0.9],\n",
    "    'xgb__min_child_weight': [1, 3, 5, 7],\n",
    "    'xgb__gamma': [0, 0.1, 0.2, 0.3],\n",
    "    'xgb__reg_alpha': [0, 0.1, 1, 10],\n",
    "    'xgb__reg_lambda': [0, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Prev Best hyperparameters:\n",
    "# xgb__subsample: 1.0\n",
    "# xgb__n_estimators: 300\n",
    "# xgb__max_depth: 7\n",
    "# xgb__learning_rate: 0.1\n",
    "# xgb__colsample_bytree: 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7552c137-7698-40bd-b6ee-8a3c2a3b9e82",
   "metadata": {},
   "source": [
    "I utilized `RandomizedSearchCV` to optimize my XGBoost model's hyperparameters for predicting electricity demand. Unlike `GridSearchCV`, which exhaustively searches through a specified grid of hyperparameters, `RandomizedSearchCV` randomly samples combinations from defined distributions (`param_dist`). This approach is beneficial when the hyperparameter search space is large, as it efficiently explores a diverse range of configurations while maintaining the integrity of time series cross-validation (`tscv`). By evaluating models based on their ability to minimize mean squared error (`neg_mean_squared_error`), I aimed to identify the optimal set of hyperparameters that enhance the model's predictive accuracy and robustness across varying time periods in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d600646-4069-4d6d-897d-f479464fd920",
   "metadata": {},
   "source": [
    "### Creating and Fitting the RandomizedSearchCV\n",
    "<a id=\"Creating-and-Fitting-the-RandomizedSearchCV\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259c792-1118-4de3-9e6b-0b620e7210e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the randomized search object\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=param_space, n_iter=50, cv=tscv, verbose = 2,\n",
    "    scoring='neg_mean_squared_error', n_jobs=1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa2979-cc4b-4175-942a-bcf5741643f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the randomized search\n",
    "random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6c954-50db-43ff-9f12-a007a08108e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af410c20-5902-44e8-8469-027b2352dd18",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "### Performing Cross-Validation on the Best Model\n",
    "<a id=\"Performing-Cross-Validation-on-the-Best-Model\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e66d1-9b2e-4b08-b5b8-3cab84d257c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on the best model\n",
    "cv_scores = []\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    cv_scores.append(np.sqrt(mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f8cdfd-116a-48db-bbea-fa7c8047611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cross-validation RMSE scores: {cv_scores}\")\n",
    "print(f\"Mean RMSE: {np.mean(cv_scores)}\")\n",
    "print(f\"Standard deviation of RMSE: {np.std(cv_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68527264-f337-4bd8-a4e5-ef0312b21254",
   "metadata": {},
   "source": [
    "## Final Model Fitting and Predictions\n",
    "\n",
    "### Fitting the Best Model on the Training Data\n",
    "<a id=\"Fitting-the-Best-Model-on-the-Training-Data\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626667fc-a548-4f59-9ad9-c7adfec3923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training data\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48d9fb-5c1a-433f-94d5-273c5b8759d0",
   "metadata": {},
   "source": [
    "### Making Predictions on the Test Data using Best Model\n",
    "<a id=\"Making-Predictions-on-the-Test-Data-Using-Best-Model\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11263429-04d1-493b-810a-d341fed23466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4589d9c-6798-41aa-af64-705205f96575",
   "metadata": {},
   "source": [
    "### Saving the Best Model as a pickle file\n",
    "<a id=\"Saving-the-Best-Model-as-a-pickle-file\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc21648-d7d0-45e3-a760-4d73a407977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'model' is your trained XGBoost model\n",
    "with open('data/xgb_electricity_demand_md.pkl', 'wb') as model_file:\n",
    "    pickle.dump(best_model, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce9cfb4-ba40-471f-9ba2-233cb4bfac65",
   "metadata": {},
   "source": [
    "### Calculating and Printing Error Metrics\n",
    "<a id=\"Calculating-and-Printing-Error-Metrics\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9664fb0-a786-4045-bbd2-b969495784ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nTest set performance:\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared Score: {r2}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843a9be4-5f4e-45af-bfd7-e5fa5bcdacff",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "### Computing Feature Importance\n",
    "<a id=\"Computing-Feature-Importance\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aefb70-f471-4004-83bc-306605950bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d6915-8f6c-4ee0-92b1-f392a4913884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_names = (numeric_features + \n",
    "                 best_model.named_steps['preprocessor']\n",
    "                 .named_transformers_['cat']\n",
    "                 .named_steps['onehot']\n",
    "                 .get_feature_names_out(categorical_features).tolist())\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': best_model.named_steps['xgb'].feature_importances_\n",
    "}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafdd3a5-6770-4d5f-8104-eab61843cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.head(10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3549502-6c3d-4938-ad99-6ec563d0af17",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Making Predictions on the entire Data\n",
    "<a id=\"Making-Predictions-on-the-entire-Data\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27b8ea-0f47-4e8f-97d0-4c9e40342a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the entire dataset\n",
    "y_predicted = best_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee944d0b-0f90-4d24-aad0-2bb216f3ce66",
   "metadata": {},
   "source": [
    "### Creating a Dataset with True and Predicted Demand\n",
    "<a id=\"Creating-a-Dataset-with-True-and-Predicted-Demand\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7e7a8-cbcc-4aa6-aa1c-7e6602ed0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with the date, hour, true demand, and predicted demand\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Date': X.index,  # Use the index as the date\n",
    "    'Hour': X.Hour,\n",
    "    'True Demand': y,\n",
    "    'Predicted Demand': y_predicted,\n",
    "    'Diff': np.round(y - y_predicted)\n",
    "})\n",
    "\n",
    "# Set the index to the Date column\n",
    "results.set_index('Date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b716afe-0d34-4e58-849e-81a308754078",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "### Plotting Hourly Electricity Demand: True vs Predicted\n",
    "<a id=\"Plotting-Hourly-Electricity-Demand-True-vs-Predicted\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8685b8b-3d6b-4b7e-b38f-53e971cf3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the hourly prediction of test and predicted set\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(results.index, results['True Demand'], label='True Demand', color='blue')\n",
    "plt.plot(results.index, results['Predicted Demand'], label='Predicted Demand', color='red', linestyle='--')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Electricity Demand (MW)')\n",
    "plt.title('Hourly Electricity Demand: True vs Predicted')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd77dd-1ead-4c4b-b613-6c9244d2862a",
   "metadata": {},
   "source": [
    "## Filtering Data for Specific Day\n",
    "\n",
    "### Filtering Data for a Specific Day\n",
    "<a id=\"Filtering-Data-for-a-Specific-Day\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3b64a0-bcba-4625-a912-9b446123f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for a specific day from 2016-01-01 to 2020-12-31 e.g., '2020-07-20'\n",
    "specific_day = '2020-07-20'\n",
    "day_data = results.loc[specific_day]\n",
    "\n",
    "# Display the filtered data\n",
    "print(day_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6abda-8da0-4aed-9fa6-792bb21c2a66",
   "metadata": {},
   "source": [
    "According to the specifications, the dataset accepts input for any date (currently within the training set, with the model designed to function for future dates as well, provided all independent variables Xi are available to predict the dependent variable y_pred). The 'Diff' column indicates the discrepancy between the true and predicted values, typically below 500MW for the majority of data points. I have conducted tests on various dates, including one provided for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5750a-e9a3-4c06-9585-857e46f9f257",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Summary of Results\n",
    "<a id=\"Summary-of-Results\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07950487-eeab-4411-aebe-0da9c871a35a",
   "metadata": {},
   "source": [
    "1. The model shows high accuracy and reliability in predicting electricity demand, with a low Mean Squared Error (MSE) of 58854.42, Root Mean Squared Error (RMSE) of 242.60, Mean Absolute Error (MAE) of 160.81, R-squared score of 0.991, and Mean Absolute Percentage Error (MAPE) of 0.0109%.\n",
    "2. The feature importance analysis ranks variables based on their contribution to predicting electricity demand:\n",
    "    - demand_lag_1: Most influential, reflecting strong dependency on previous hour's demand.\n",
    "    - Cos_Hour: Significant cyclic pattern impact, likely related to daily cycles.\n",
    "    - Hour_8: Indicates importance of a specific hour, possibly a peak consumption period.\n",
    "    - EWMA_6: Captures smoothed demand trends over time.\n",
    "    - Others, like Hour_7, Hour_23, Sin_Hour, Hour_0, IsWeekend_0, and demand_lag_2, also contribute but to a lesser extent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ef3ec-69ee-4044-bbcd-91a58aceb854",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "<a id=\"Future-Work\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f95a03-6457-49be-869d-5ed04d2dfea4",
   "metadata": {},
   "source": [
    "1. **Incorporate Additional External Variables**: Consider integrating economic indicators and population demographics alongside weather data to enhance prediction accuracy and capture broader influences on electricity demand.\n",
    "\n",
    "2. **Advanced Modeling Techniques**: Explore the application of advanced algorithms such as LSTM networks to improve the model's ability to handle complex temporal dependencies and enhance forecasting accuracy.\n",
    "\n",
    "3. **Spatial Analysis**: Extend the analysis to include regional variations in electricity demand by leveraging geographic data. This approach can provide tailored predictions and optimize resource allocation strategies based on localized demand patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44a09e-79ca-481f-8212-0269ceea29a5",
   "metadata": {},
   "source": [
    "### References\n",
    "<a id=\"References\"></a>\n",
    "- **Canada holidays**: Available at: [https://pypi.org/project/holidays/](https://pypi.org/project/holidays/).\n",
    "\n",
    "- **Canada Seasons**: Available at: [https://www.durhamimmigration.ca/en/moving-to-durham-region/weather---four-seasons.aspx](https://www.durhamimmigration.ca/en/moving-to-durham-region/weather---four-seasons.aspx).\n",
    "\n",
    "- **XGBoost Documentation**: Official XGBoost Documentation. Available at: [https://xgboost.readthedocs.io/](https://xgboost.readthedocs.io/).\n",
    "\n",
    "- **Edgecom-AI Data Source**: Data sourced from edgecom-ai for the electricity demand prediction project.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
